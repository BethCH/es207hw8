---
title: "ECH_ES207HW8"
author: "Beth Clifton Holcomb"
date: "4/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. What is spurious correlation? What is a “lurking”" variable? Why should we worry about it in the context of predicting loads from concentrations? What is an example in your research where spurious correlation and/or lurking variables may be of concern?

**Spurious Correlation** is a seemingly present correlation between two variables that are actually unrelated. There's a fantastic website that shows obsurd examples of this : http://tylervigen.com/page?page=1 . Though there are great examples like the graph of pool drownings vs Nicolas Cage movies, it can happen in environmental and ecological data. in these cases, it may be far more difficult to spot, which is the reason and extreme value of statistical tests

**"Lurking" Variable** is a variable that may explain / hide the explainatory power of the relationship(s) between the independant and dependant variable(s). 

Lurking variables can be an issue in predicting any variable because if the appropriate variables are not used/included to make the prediction (exluding lurking variables/including lurking variables) the predictioins may be inaccurate. the scale of error is determined by the power of a lurking variable.

In my own research, I am far more concerned with spurious correlation than lurking variables. though they are both issues, i am trying to test the strength of the relationship between two bodies of literature that lack prior data. In this case, the interpretation of the resulting data could lead to a seemingly valuable relationship between sense of place and food sovereignty literature, when in reality the results may just follow similar trends without any causation or interdependence. Such concerns are valuable in assessing the validity of results. 

## 2. Make a prediction about the outcome of this analysis. How will the results using C compare to those using L as the regression’s response variable?

if L is calculated based on Q, the concern for spurious correlation is justifiable. i would imagine that data created using another data set would show corriliation to the foundation data set. the argument to use C instead of L to determine a relationship seems more sound. i would predict more variation between C and Q than would be found between L and Q.



## 3. Provide two scatterplots using ggplot. The first should be discharge (Q) as the independent variable versus Load as the dependent variable. The second should be discharge (Q) as the independent variable versus Concentration as the dependent variable. Please make these plots publication quality. Make sure to provide 1) Good figure captions that describe what is graphed, include important experimental details, and match the graphic; 2)

```{r}
library(tidyverse)
il_river <- read_csv("illinois_p.csv", col_names = TRUE, na = c("NA", "n/p", "n/a"), guess_max = 30000)
il_river
```

```{r}
require(tidyverse)
d <- tibble(
  Discharge = il_river$`Q`,
  Load = il_river$`Load`)
d <- na.omit(d)
fit <- lm(Discharge~Load, data = d)
d$predicted <- predict(fit)
d$residuals <- residuals(fit)
a <- d %>% 
  ggplot(aes(x = Discharge, y = Load)) +
  theme_bw() +
  geom_point(alpha = 0.2) + labs(title = "Figure 1. Discharge to Load", caption = "This plot displays Stream Discharge compared to Phosphorous Load of 
 the Illinois River. Because discharge is used to calculate load, there 
 may be a misleading relationship between the two readings.")
a
```


```{r}
require(tidyverse)
d <- tibble(
  Discharge = il_river$`Q`,
  Concentration = il_river$`Conc`)
d <- na.omit(d)
fit <- lm(Discharge~Concentration, data = d)
d$predicted <- predict(fit)
d$residuals <- residuals(fit)
b <- d %>% 
  ggplot(aes(x = Discharge, y = Concentration)) +
  theme_bw() +
  geom_point(alpha = 0.2) + labs(title = "Figure 2. Discharge to Concentration", caption = "This plot displays Stream Discharge compared to Phosphorous Concentration of the 
Illinois River. Stream Discharge and Phosphorous concentration are measured seperately. Unlike 
Figure 1, the indenpendence of these two variables reduces the liklihood of misleading through this plot.")
b
```

```{r}
# i'm not sure why this doesn't work
par(mfrow = c(1,2)); plot(a); plot(b)
```



```{r}
LvQ <- lm(log(Load)~log(Q), data = il_river)
CvQ <- lm(log(Conc)~log(Q), data = il_river)
summary(LvQ)
```

```{r}
summary(CvQ)
```

```{r}
require(modelr)
dat <- il_river %>% 
  spread_predictions(LvQ, CvQ)
dat
```

```{r}
dat.g <- dat %>% 
  gather_predictions(LvQ, CvQ)
head(dat.g)
dat.r <- dat %>% 
  spread_residuals(LvQ, CvQ)
head(dat.r)
```

```{r}
Load.scale <- function(Q, C){2.7*Q*C}
dat$Load.fromC <- Load.scale(dat$Q, exp(dat$CvQ))
dat
```

```{r}
ggplot(dat, aes(x = Load, y = exp(LvQ))) + geom_point(alpha = 0.2) + 
  stat_smooth(method = "lm", se = F) + geom_abline(linetype = "dashed")
```



## 4. Write a function that calculates each of the above statistics, and returns a tibble that where the column heading is the model name, and each row is a statistic. Apply the function to both models.

Function = calculates MAE and RMSE for 2 models (LvQ & CvQ) & make a tibble

MAE = mae(model, data)
RMSE = rmse(model, data)

both models = 1) LvQ 2) CvQ
LvQ = log(Load)~log(Q)
CvQ = log(Conc)~log(Q)


**Come back to this!! It's not finished**

```{r}
LvQ <- lm(log(Load)~log(Q), data = il_river)
CvQ <- lm(log(Conc)~log(Q), data = il_river)
a <- tibble(
  Load = dat$Load,
  Concentration = dat$Conc)
a <- na.omit(a)
a$RMSE_LvQ <- rmse(LvQ, dat)
a$MAE_LvQ <- mae(LvQ, dat)
a$RMSE_LvQfit <- predict(fit)
a$MAE1_LvQfit <- residuals(fit)
a$RMSE_CvQ <- rmse(CvQ, dat)
a$MAE_CvQ <- mae(CvQ, dat)
a$RMSE_CvQfit <- predict(fit)
a$MAE_CvQfit <- residuals(fit)
head(a)
```





## 5. What is the conclusion to the controversy? Which model better makes a better predction? The model using Q as the dependent variable, or the model using C as the dependent variable. Explain your reasoning.

For a linear model, the Load seems to fit better considering the R-squared value and p-value. considering the intercepts, the Load vs. Q all sits on the same side of zero. I am very suspicious of supporting the Load model but just using the available numbers, it seems the better fit.  

## 6. Find some transformations for the above example which might improve on the natural log transformation for x and/or y. Obvious candidates include the ladder of power transformation, or log10. ’

```{r}
b <- ggplot(dat, aes(x = log(dat$Load), y = dat$Q)) +
  theme_bw() + geom_point(alpha = 0.2)
b
```

```{r}
c <- ggplot(dat, aes(x = log(dat$Q), y = dat$Load)) +
  theme_bw() + geom_point(alpha = 0.2)
c
```

```{r}
c <- ggplot(dat, aes(x = log(Q), y = log(Load))) + 
  theme_bw() + geom_point(alpha = 0.2)
c
```

https://rdrr.io/cran/rcompanion/man/transformTukey.html
```{r}
library(rcompanion)
transformTukey(dat$Q, start = 0, end = 100, int = 10,
  plotit = TRUE, verbose = FALSE, quiet = FALSE, statistic = 1,
  returnLambda = FALSE)
```

```{r}
transformTukey(dat$LvQ, start = -10, end = 10, int = 0.025,
  plotit = TRUE, verbose = FALSE, quiet = FALSE, statistic = 1,
  returnLambda = FALSE)
```

```{r}
transformTukey(dat$CvQ, start = -10, end = 10, int = 0.025,
  plotit = TRUE, verbose = FALSE, quiet = FALSE, statistic = 1,
  returnLambda = FALSE)
```

```{r}
transformTukey(dat$Q, start = -10, end = 10, int = 0.025,
  plotit = TRUE, verbose = FALSE, quiet = FALSE, statistic = 1,
  returnLambda = FALSE)
```


```{r}
library(gridExtra)
a1 <- qplot(x = CvQ, data = dat, binwidth = .025)
b1 <- qplot(x = log10(CvQ + 1), data = dat, binwidth = .025)
c1 <- qplot(x = sqrt(CvQ + 1), data = dat, binwidth = .025)

grid.arrange(a1, b1, c1, nrow = 1)
```

```{r}
library(gridExtra)
a2 <- qplot(x = LvQ, data = dat, binwidth = .025)
b2 <- qplot(x = log10(LvQ + 1), data = dat, binwidth = .025)
c2 <- qplot(x = sqrt(LvQ + 1), data = dat, binwidth = .025)

grid.arrange(a1, b1, c1, nrow = 1)
```


## 6a. What is a good transformation of Q to use in estimating L and C? What is a good transformation to use for L and C? Show evidence for your work. *Note! There is no “best” transformation, but there are some good ones.

The transformations visualized in the previous question and sampled below indicate the transformTukey command transforms the data to a normal distribution in both CvQ and LvQ as well as Q on its own. The results for the Tukey tranform were far closer to normally distributed than for the log10 tranformation. the square root tranformation was not useful in any of the variables.

```{r}
transformTukey(dat$Q, start = -10, end = 10, int = 0.025,
  plotit = TRUE, verbose = FALSE, quiet = FALSE, statistic = 1,
  returnLambda = FALSE)
```

```{r}
a2
```



## 6b. Describe your preferred model and indicate some reasons you might be oncerned about it. What are steps you could take to improve this model in future work?

I prefer the CvQ model for quality sake. The models currently result in LvQ being a better fit but from a quality assessment, i do not trust that the variables are independent enough from each other to make qualitiy results. Because the models indicate LvQ a better fit, the CvQ model should be evaluated and modified to reach a better fit for predictions before it is used. This may include additional factors in the model or a non-normal model. 



** come back to this
## 6c. What does it tell you about Phosphorous transport in the Illinois River?

This lesson is difficult because i'm still learning how to interprete results from different models. it seems that Phosphorous is low and there are individual points of high concentration which seem to be resulting in issues in the model. But, these seemingly outlying events in concentration are not seen in the load as much. I'm not really sure what i'm being told about Phosphorous transport from this data. 






PO4 covaries with P (SRP)
SRP analysis can overestimate PO$

** Determine how much variation in PO4 is explained by SRP
** variables: SRP @ 25th, 50th, 75th, 9th %ile
              max concentration events 
              log transformed Mehicle 3 values for phosphorus columns for 3 depths
              
** obejective: regress the PO4 Column (by: %ile to source / %ile to surface runoff / %ile to tile drainage) against log transformed soil test P for each depth
1. %ile to source vs P 0-2in depth
2. %ile to source vs P 0-4in depth
3. %ile to source vs P 0-8in depth
4. &ile to surface runoff vs 3 P options
5. %ile to tile drainage vs 3 P options
** data: Event_data_log.xlsx

```{r}
library(tidyverse)
library(readxl)
psoil <- read_excel("Event_data_log.xlsx")
head(psoil)
```

```{r}
psoil.g <- gather(psoil, key = "depth", value = "soiltest", log_0_2, log_0_4, log_0_8)
head(psoil.g)
```


## 7. Re-write the code as above, but use the piping function preferred by the tidyverse. Prove to me you get the same output.




```{r}
library(tidyverse)
psoil %>% gather(key = "depth", value = "soiltest", log_0_2, log_0_4, log_0_8)
head(psoil)
```

```{r}
psoil[45,]
```


```{r}
psoil.g[45,]
```


```{r}
psoil.g$depth <- substr(psoil.g$depth, start = 7, stop = 7)
psoil.g
```

```{r}
ggplot(psoil.g, aes(x = soiltest, y = log_PO4, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```

```{r}
psoil.nested <- psoil.g %>% 
  group_by(depth, source, percentile) %>% 
  nest()
psoil.nested
```

```{r}
psoil.nested$data[1]
```



## 8. Using the indexing techniques for list columns, print for me the third number in the vector of log_PO4 from depth = 2, source = tile, percentile = 75.

```{r}
# print the log_PO4 for depth = 2, source = tile, precentile = 75
psoil.nested[[4]][8]
psn <- psoil.nested[[4]][8]
psn
```

```{r}
# i dont know why this doesn't work
#psn[2,3]
```

```{r}
st_vs_po4 <- function(df) {
  lm(log_PO4 ~ soiltest, data = df)
}
st_vs_po4(psoil.nested[[1, "data"]])
```

```{r}
psoil.nested <- psoil.nested %>% 
   mutate(lm.fit = map(data, st_vs_po4))
psoil.nested
```

```{r}
psoil.nested$lm.fit[1]
```


## 9. Write another function calculating the summary of the lm and map() mutate() it to add to the object psoil.nested. Call that item “lm.summary”. Show me your results and prove to me it works.

```{r}
st_vs_po4_2 <- function(df) {
  summary(lm(log_PO4 ~ soiltest, data = df))
}
st_vs_po4(psoil.nested[[1, "data"]])
```

```{r}
psoil.nested <- psoil.nested %>% 
   mutate(lm.summary = map(data, st_vs_po4_2))
psoil.nested
```

```{r}
psoil.nested$lm.summary[1]
```

```{r}
require(broom)
psoil.nested <- psoil.nested %>% 
  mutate(lm.coeffs = map(lm.fit, tidy)) %>% 
  mutate(lm.stats = map(lm.summary, glance))
psoil.nested
```

```{r}
psoil.nested$lm.stats[[1]]
```

```{r}
psoil.stats <- psoil.nested %>% 
  dplyr::select(depth, source, percentile, lm.stats) %>% 
  unnest(lm.stats)
psoil.stats
```

```{r}
psoil.coeffs <- psoil.nested %>% 
  dplyr::select(depth, source, percentile, lm.coeffs) %>% 
  unnest(lm.coeffs)
psoil.coeffs
```



## 10. Make some graphs that enable Dr. Duncan to visualize the results of the different regressions and compare the regression results for different depths, sources, and quantiles.

```{r}
library(ggplot2)
ggplot(psoil.coeffs, aes(x = estimate, y = statistic, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```

```{r}
library(ggplot2)
ggplot(psoil.coeffs, aes(x = estimate, y = std.error, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```

```{r}
library(ggplot2)
ggplot(psoil.stats, aes(x = r.squared, y = statistic, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```

```{r}
library(ggplot2)
ggplot(psoil.stats, aes(x = r.squared, y = sigma, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```

```{r}
library(ggplot2)
ggplot(psoil.stats, aes(x = r.squared, y = p.value, color = percentile)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(source~depth)
```


## 11. What can you conclude about the ability of SRP to measure PO4? Make sure to address why Dr. Duncan divided her data into different quantiles for analysis, and how that influences the results. What are the limitations of the analysis?

considering the very small p-values and the low standard errors, the results indicate SRP measures PO4 fairly well. There are also indicators of linear relationships in some of the graphs. Comparing the quantiles, there's more information which could lead to more accuracy in the interpretation of data. the standard errors for the 50th percentile, or the middle point of the measurements, are the lowest as are the p-values (even though they are all super low). if the data had only been single points, instead of quantiles, there would not be variation. Each reading would only be a single point, roughly the 50th percentile point. This loss of sensitivity at each depth would mean the loss of range for each depth and location. Though the quantiles provide more detail, the analysis is only as strong as the data. there are also limitations in that in some of the estimates have substantially higher than ideal standard errors and the resulting statistic for the intercept of the estimates match in patern and linear expression to the qunatile soiltests but i cannot conclusively say that the results are conclusive. 


## 12. Often, we use regression to remove the primary effect of variation on a dataset, and then we examine the residuals for other effects. We dont have any other covariates to analyze in this dataset, so conduct a thought experiment. In a few sentences, outline for me the next steps in this analysis you would take to do such an exercise. Hint, see the “Model” chapter of R 4 Data Science to guide your answer.

Analyzing the data results from a single mutation is a great place to start but the data interpretation takes many more steps. Chapter 18 in Wickham & Grolemund discuss the value and application of taking the data and applying a family of models in, for example the linear models, to determine the best fitting model. Once the best fitting model is determined, which is done by finding the model with the least distance between the data points and the model, the specific model can be applied to the data. the results from this application include residuals and outliers which are to be analyzed. the residuals tell just how well a model fits. after the model is selected, relationships and interactions in the data are then analyzed. These next steps go into assessing the approporiatness of appling tranformations within the model. the results should be a model that best describes the behavior in the data. if that is not the result, the model is bad. 


streamflow -> concentration
x & y & z & a & q -> groundwater contamination 

147 soild profile observations
1.Clay = clay content - weight % of the mineral fine earth (<2 mm)

2.CEC = cation exchange capacity - cmol+/(kg soil) - controls leaching

3.OC = organic carbon - volume % of the fine earth

4.e & n = Earth and NOrth UTM Coordinates (UTM Zone 32N, WGS 84 in meters)

5.elev = elevation in meters

6.zone = Agro-ecological zone

7.wrb1 = Reference soil group

8.LC = land cover type

```{r}
soil <- read_csv("cameroon_soil.csv")
soil
```

```{r}
pairs(~Clay1 + OC1 + CEC1, data = soil)
```

```{r}
topsoil <- dplyr::select(soil, CEC1, Clay1, OC1)
cov(topsoil)
```

```{r}
cor(topsoil)
```





## 13. In 3-5 sentences, interpret these results for me. Check the documentation on the functions for assistance as needed.

Starting with the visualization of the data, there seems to be a linear relationship in each of the sets worth further investigation. Assessing the covariation between the variables, there's varying interpretations avaiable in the text and through online resources as to what exactly the high cov results indicate but there is clear concensus that the large positive numbers indicate high covariance. This is also supported by the 194.2 results for the Clay1 to Clay1 though not so with the OC1 to OC1. As for the correlation, the Clay1 and CEC1 have the lowest correlation with .557 but each of the results align with the conclusions of the visual interpretation that there are relationships between each of the pairs of variables. 



cor (CEC1 to OC1) = .74
cor (CEC1 to Clay1) = .55
## 13. Given this information, if you had to select a single best predictor of CEC in topsoil, which variable would you select?

OC1 has higher correlation results and more focused visual linear relationship with CEC than the Clay1 variable. After controlling for residuals, the strength of the argument for using OC rather than Clay is strengthened. I would use the OC1.



```{r}
fit1 <- ggplot(topsoil, aes(x = Clay1, y = CEC1)) + 
  geom_point() + 
  geom_hline(yintercept=mean(topsoil$CEC1)) + 
  geom_vline(xintercept=mean(topsoil$Clay1)) +
  ylab("Percent Clay") +
  xlab("CEC cmol+/(kg soil)") +
  ggtitle("Clay vs. CEC at 0-10 cm")
fit2 <- ggplot(topsoil, aes(x = residuals(lm(CEC1~OC1, data = topsoil)), y = residuals(lm(Clay1~OC1, data = topsoil)))) + 
  geom_point() + 
  geom_hline(yintercept=mean(residuals(lm(CEC1~OC1, data = topsoil)))) + 
  geom_vline(xintercept=mean(residuals(lm(Clay1~OC1, data = topsoil)))) +
  ylab("Residuals, % Clay vs. % OC") +
  xlab("Residuals, CEC vs. OC") +
  ggtitle("Partial Corr. of Clay vs. CEC \n after correcting for OC")
```

```{r}
library(cowplot)
plot_grid(fit1, fit2, labels = "AUTO")
```

```{r}
library(ppcor)
pcor(topsoil, method = "pearson")
```

```{r}
library(ppcor)
pcor(topsoil, method = "kendall")
```

```{r}
library(corrplot)
mycor <- cor(topsoil)
corrplot(mycor, method = "number")
```

```{r}
lm.null <- lm(CEC1 ~ 1, data = topsoil); summary(lm.null)
```

```{r}
lm.cec.oc <- lm(CEC1 ~ OC1, data = topsoil); summary(lm.cec.oc)
```

```{r}
lm.cec.clay <- lm(CEC1 ~ Clay1, data = topsoil); summary(lm.cec.clay)
```

```{r}
lm.cec.clay.oc <- lm(CEC1 ~ Clay1 + OC1, data = topsoil); summary(lm.cec.clay.oc)
```

## 14. Using tidyverse and the broom package, clean up the lm results from above. Make a summary table that has the following columns: r square; adjusted r square, sigma, statistic, p.value, df, logLik, AIC, BIC, deviance, df.residual. Each row should be each of the different linear models. Show your work.

```{r}
lm.null <- lm(CEC1 ~ 1, data = topsoil); summary(lm.null)
lm.cec.oc <- lm(CEC1 ~ OC1, data = topsoil); summary(lm.cec.oc)
lm.cec.clay <- lm(CEC1 ~ Clay1, data = topsoil); summary(lm.cec.clay)
lm.cec.clay.oc <- lm(CEC1 ~ Clay1 + OC1, data = topsoil); summary(lm.cec.clay.oc)
```


```{r}
glance(lm.null)
```

```{r}
ts.models <- bind_rows(list(lm.null = glance(lm.null), lm.cec.oc = glance(lm.cec.oc), lm.cec.clay =glance(lm.cec.clay), lm.cec.clay.oc = glance(lm.cec.clay.oc)), .id = "id")
ts.models
```



```{r}
lms <- list(null=lm.null, cec.clay = lm.cec.clay, cec.oc = lm.cec.oc, cec.clay.oc = lm.cec.clay.oc)
lms.coeffs <- lapply(lms, tidy)
lms.coeffs
```

## 15. Using your answer from 14 and looking at the lm coefficients above, answer the above two questions. 1) How much explanation is gained by adding Clay? Is this a statistically signafigant increase? Support your answers with quantitative results.

In comparing the lm.cec.oc to the lm.cec.clay results to each other, there is substantially less value to the .clay results compared to the .oc results. the r2 and adjusted r2 values support the value of the .oc results more vigorously than the .clay results. As do the test statistics and the p-values. With the addition of clay to the last tests, lm.cec.clay.oc, these results mirror those of the lm.cec.oc indicating little change with the additional variable. with the independent results between .oc and .clay and the supporting evidence when .clay is added to the .oc, it is apparent that clay plays a minimal role in clay's statistical value.  

```{r}
lms.coeffs.notalist <- Map(cbind, lms.coeffs, model = names(lms))
lms.coeffs.notalist <- bind_rows(lms.coeffs.notalist)
lms.coeffs.notalist
```

## 16. What is the adjusted R square of the 4 different lms?

```{r}
ts.models[,3]
```
Adjusted R2:
lm.null = 0.0
lm.cec.oc = .5489
lm.cec.clay = .3066
lm.cec.oc.clay = .5662

## 16. Define the AICc statistic. What is the function for calculating the AICc statistic in R?

AICc is the second order AIC or Alaike's Information criterion. AIC is the maximum liklihood fit of a model considering overall fit and number of parameters. AIC is typically used, unless there is a very low number of samples in which case the AICc is used. 

Function:
AICc(mod, return.K = FALSE, second.ord = TRUE, nobs = NULL, …)

Source: https://www.rdocumentation.org/packages/AICcmodavg/versions/2.2-1/topics/AICc

## 17. Find the AIC and the BIC of each of the 4 lm models. Make a side-by-side bar plot visualizing the AIC and BIC for each of the 4 models (so, x-axis = model, y-axis = test statistic value, color = test statistic type).

```{r}
ts.models[,9:10]
```

```{r}
library(ggplot2)
library(reshape2)
AIC <- ts.models$AIC
BIC <- ts.models$BIC
id <- ts.models$id
df1 <- data.frame(AIC, BIC, id)
df1
df2 <- melt(df1, id.vars = 'id')
df2
ggplot(df2, aes(x = id, y = value, fill = variable)) + geom_bar(stat = 'identity', position = 'dodge', colour = "black") + theme(axis.text.x = element_text(angle = -35))
```

Source: https://stackoverflow.com/questions/42820677/ggplot-bar-plot-side-by-side-using-two-variables

## 18. Which of the 4 lms is best? Why?

The lm.cec.clay.oc has the lowest of both AIC and BIC but it is only slightly lower than the lm.cec.oc. With the other information, i would feel far more comfortable using the lm.cec.oc as the best because clay has such a low impact. 



```{r}
a <- anova(lm.cec.clay.oc, lm.cec.clay)
a
```
```{r}
diff(a$RSS)/a$RSS
```

```{r}
b <- anova(lm.cec.clay.oc, lm.cec.oc)
b
```

```{r}
diff(b$RSS)/(b$RSS)
```

```{r}
ts.anovas <- bind_rows(list(a,b))
ts.anovas
```


## 19. Plot the regression diagnostic plots, and the predicted (fitted) versus observed values for all 4 lms. For the diagnostic plots, use par(mfrow) and plot 4 diagnostic plots in a 2 x 2 grid. Include residuals vs fitted, normal q-q, scale-location, and residuals versus leverage.

```{r}
lm1 <- lm(CEC1 ~ 1, data = topsoil)
#lm.null
lm2 <- lm(CEC1 ~ OC1, data = topsoil)
#lm.cec.oc
lm3 <- lm(CEC1 ~ Clay1, data = topsoil)
#lm.cec.clay
lm4 <- lm(CEC1 ~ Clay1 + OC1, data = topsoil) 
#lm.cec.clay.oc
```

```{r}
# plot the 4 diagnostics for lm.null
par(mfrow = c(2,2));
plot(lm1, which = 1) + plot(lm1, which = 2) + plot(lm1, which = 3) + plot(lm1, which = 4)
```

```{r}
lm.plots <- function(x){
  par(mfrow = c(2,2));
plot(x, which = 1) + plot(x, which = 2) + plot(x, which = 3) + plot(x, which = 4)
}
#lm.null
lm.plots(lm1)
```

```{r}
#lm.cec.oc
lm.plots(lm2)
```

```{r}
#lm.cec.clay
lm.plots(lm3)
```

```{r}
#lm.cec.clay.oc
lm.plots(lm4)
```



## 20. Given the results from the ANOVA and the proportional difference in the residual sum of the squares, as well as the visualizations, what is the probability that the difference in the models is due to chance? (Or, put another wasy, can we reject our null hypothesis that the extra information from the clay content does not really improve the model)? Explain your reasoning.

```{r}
ts.models
```

```{r}
ts.anovas
```

Considering the AIC, the BIC, and the anova results, the null must be rejected. There is a slightly significant (.01) partial f test and both AIC and BIC are lower for the model that included clay. While the model analysis showed very similar results between the model that did include clay with the model that did not include clay, there is enough evidence that the mild difference is enough to include clay. I am very skeptical of this conclusion but using the information i have gathered so far between analysis and information on analysis interpretation, i cannot not reject the null. 


## 21. Given the above results, should we eliminate any of the variables?

looking at the AIC values, the clay variable heavily increases AIC when it is removed to the model. OC1 and OC2 removal in the last iteration has seemingly little impact and I would feel confident in investigating the value in removing those two variables. Clay1 is a similar scenario.



```{r}
lms <- step(lm(Clay5 ~ Clay1 + CEC1 + OC1 + Clay2 + CEC2 + OC2, data = soil))
```



## 21. Under what case is AIC most improved? Explain your reasoning. (Note - there is not one correct answer here).

Comparing the start AIC to the variation in each model, the first (Clay5 ~ Clay1 + CEC1 + Clay2 + CEC2 + OC2) has the least variation with the 420.7 as the start and 501.33 as the AIC when Clay2 is removed. This model also has the lowerst AIC when Clay2 is removed which leads me to believe that though the variables discussed in the previous question may be removed without much diviation from quality, there may still be a bit of value for these variables. 



```{r}
soil$zone <- as.factor(soil$zone)
lm5z <- lm(Clay5 ~ zone, data = soil); summary (lm5z)
lm51 <- lm(Clay5 ~ Clay1, data = soil); summary(lm51)
lm5z1 <- lm(Clay5 ~ zone + Clay1, data = soil); summary(lm5z1)
```

## 22. How much of the variation in subsoil clay is explained by the zone? By the topsoil clay? by both together? Is the combined model better than individual models? How much so?

Zone explains roughly 50% (.5126) of the variation in subsoil clay.
Topsoil clay explains roughly 80% (.8047) of the variation in subsoil clay.
The combined model explains roughly 83% (.8296) of the variation in but that is only 2% higher than the topsoil model alone. This starting point of model assessment does not support a conclusive dicision on the quality of the model. further investigation would be needed to make that decision.

## 23. In the parallel regression model (topsoil clay and zone as predictors), what are the differences in the means between zones?

the maximum difference between zones is almost 6. The highest is 5.694 and the lowest is -.659

the difference between zone 2 and 3 = 3.444
the difference between zone 3 and 4 = 1.592
the difference between zome 2 and 4 = 5.035



## 24. What is the slope of the linear regression, after accounting for the zones? How does this compare with the slope of the linear regression not considering zones?

when considering zone, the slope of the line combination of estimates for each of zones and clay1. with this in mind, the slope is 2.335. this linear model would correct for the up and own curvature that would be expressed for each of the variable slopes interpreted through each of the estimates. 

when removing the consideration of zones, the slope of the line for clay1 is .82891. 

these slopes vary heavy from one another indicating they are diversly different linear models of the data. because of this deviation, these two models would need to be considered against eachother and with other variables to assess their overall value.


## 25. Are all predictors in the combined model (topsoil clay and zone as predictors) significant? (Hint: look at the probability of the t-tests.)

Clay1 and zone2 are significantly smaller than 1. zone 3 is smaller but not significantly so and zone4 does not indicate any significance as a predictor. 



```{r}
stem(residuals(lm5z1))
res.low <- which(residuals(lm5z1) < -12)
res.high <- which(residuals(lm5z1) > 9)
soil[res.low,]
```

```{r}
soil[res.high,]
```

```{r}
predict(lm5z1)[res.low]
```

```{r}
predict(lm5z1)[res.high]
```



## 25. Plot these parellel regression lines

```{r}
lm5z <- lm(Clay5 ~ zone, data = soil)
lm51 <- lm(Clay5 ~ Clay1, data = soil)
lm5z1 <- lm(Clay5 ~ zone + Clay1, data = soil)
```

```{r}
plot(Clay5 ~ zone, data = soil)
abline(lm5z)
```

```{r}
plot(Clay5 ~ Clay1, data = soil)
abline(lm51)
```

```{r}
plot(Clay5 ~ zone + Clay1, data = soil)
abline(lm5z1)
```




```{r}
library(ggplot2)

ggplot(soil, aes(x = zone, y = Clay5)) + 
  geom_point(shape = 1) + 
  geom_smooth(method = lm)
```

```{r}
library(ggplot2)

ggplot(soil, aes(x = Clay1, y = Clay5)) + 
  geom_point(shape = 1) + 
  geom_smooth(method = lm)
```

```{r}
library(ggplot2)

ggplot(soil, aes(x = zone + Clay1, y = Clay5)) + 
  geom_point(shape = 1) + 
  geom_smooth(method = lm)
```




```{r}
library(car)
vif(lm(Clay5 ~ Clay1 + CEC1 + OC1 + Clay2 + CEC2 + OC2, data=soil))
```



## 25. Recompute the VIF, take each one out separately. Show the results.

```{r}
vif(lm(Clay5 ~ CEC1 + OC1 + Clay2 + CEC2 + OC2, data=soil))
vif(lm(Clay5 ~ Clay1 + OC1 + Clay2 + CEC2 + OC2, data=soil))
vif(lm(Clay5 ~ Clay1 + CEC1 + Clay2 + CEC2 + OC2, data=soil))
vif(lm(Clay5 ~ Clay1 + CEC1 + OC1 + CEC2 + OC2, data=soil))
vif(lm(Clay5 ~ Clay1 + CEC1 + OC1 + Clay2 + OC2, data=soil))
vif(lm(Clay5 ~ Clay1 + CEC1 + OC1 + Clay2 + CEC2, data=soil))
```

```{r}

```



## 26. What is the best model to predict Clay 5? Why?
Considering all variables,

Clay5 ~ CEC1 + OC1 + Clay2 + CEC2 + OC2
and
Clay5 ~ Clay1 + CEC1 + OC1 + Clay2 + OC2

have the lowest VIF among all the options. Because VIF measures for multicollinearity, having low VIF values is idea. Because of the very high (>10) VIF values in several of the models for Clay1, i would judge the Clay5 ~ CEC1 + OC1 + Clay2 + CEC2 + OC to be the best option of these. Clay1 and Clay2 seem to be very correlated. these two variables have very high VIF when they are both in the model. 



